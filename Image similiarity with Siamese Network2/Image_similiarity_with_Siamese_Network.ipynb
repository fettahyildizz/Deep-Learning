{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image similiarity with Siamese Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Sea1bWrChU"
      },
      "source": [
        "import numpy as np \n",
        "from tensorflow import keras\n",
        "import os\n",
        "import cv2\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import applications\n",
        "from keras.preprocessing import image as image_utils\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D,MaxPooling2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Flatten\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.image as mpimg\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
        "\n",
        "\n",
        "def load_and_scale(img):\n",
        "  image = image_utils.load_img(img, target_size = ())\n",
        "\n",
        "train_labels = []\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    \n",
        "    for idx, filename in enumerate(os.listdir(folder)):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "        if img is not None:\n",
        "            images.append(img)          \n",
        "\n",
        "            temp = filename[5:]\n",
        "            temp = int(temp[:2]) \n",
        "                      \n",
        "            train_labels.append(temp)\n",
        "\n",
        "    return images\n",
        "\n",
        "def make_pairs(images,labels):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "  numClasses = len(np.unique(labels))\n",
        "  idx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "  for idxA in range(len(images)):\n",
        "    currentImage = images[idxA]\n",
        "    label = int(labels[idxA])\n",
        "    \n",
        "    idxB = np.random.choice(idx[int(label)])\n",
        "    posImage = images[idxB]\n",
        "    posImage = images[idxB]\n",
        "\n",
        "    pairImages.append([currentImage, posImage])\n",
        "    pairLabels.append([1])\n",
        "\n",
        "    negIdx = np.where(labels != label)[0]\n",
        "    negImage = images[np.random.choice(negIdx)]\n",
        "\n",
        "    pairImages.append([currentImage, negImage])\n",
        "    pairLabels.append([0])\n",
        "\n",
        "  return (np.array(pairImages), np.array(pairLabels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB5WaslbwJAV"
      },
      "source": [
        "\n",
        "train_dataset = []\n",
        "test_dataset = []\n",
        "train_dataset = load_images_from_folder(\"/content/drive/MyDrive/train3\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBvioR0W2Qm"
      },
      "source": [
        "for i in range(len(train_dataset)):\n",
        "  train_dataset[i] = cv2.resize(train_dataset[i], dsize=(100, 100), interpolation=cv2.INTER_CUBIC)  \n",
        "  train_dataset[i] = keras.applications.xception.preprocess_input(\n",
        "    train_dataset[i], data_format=None\n",
        ")\n",
        "\n",
        "train_dataset_np = np.asarray(train_dataset).astype(np.float32)\n",
        "\n",
        "\n",
        "train_labels_np = np.asarray(train_labels).astype(np.int)\n",
        "\n",
        "\n",
        "label_dict = {99:5, 98:13, 97:25, 96:32,94:34,93:36,92:38,91:40,90:48,89:50,88:52,86:57,85:60,84:61,83:65,80:76,79:78}\n",
        "train_labels_np_2 = np.copy(train_labels_np)\n",
        "for k, v in label_dict.items(): train_labels_np_2[train_labels_np==k] = v\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFyyinUkL-WQ"
      },
      "source": [
        "import itertools\n",
        "b = list(itertools.combinations(train_labels_np_2,2))\n",
        "pairLabels = []\n",
        "for i in range(len(b)):\n",
        "  if b[i][0]==b[i][1]:\n",
        "    pairLabels.append(1)\n",
        "  else:\n",
        "    pairLabels.append(0)\n",
        "\n",
        "\n",
        "pairLabels = np.array(pairLabels)\n",
        "pairLabels = pairLabels[:30000]\n",
        "\n",
        "pairImages = list(itertools.combinations(train_dataset_np,2))\n",
        "pairImages = pairImages[:30000]\n",
        "pairImages_np = np.array(pairImages)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLklXuBPN7NQ"
      },
      "source": [
        "def make_pairs2(images,labels):\n",
        "  pairImages = []\n",
        "  pairLabels = []\n",
        "  temp=[]\n",
        "  temp = list(itertools.combinations(labels,2))\n",
        "  for i in range(len(temp)):\n",
        "    if temp[i][0] == temp[i][1]:\n",
        "      pairLabels.append(1)\n",
        "    else:\n",
        "      pairLabels.append(0)\n",
        "  \n",
        "  pairImages = list(itertools.combinations(images,2))\n",
        "  return (np.array(pairImages), np.array(pairLabels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PY_hraLbsqQ"
      },
      "source": [
        "inp_shape=(100,100,3)\n",
        "def build_siamese_model2(inputShape = inp_shape, embeddingDim=128):\n",
        "  # specify the inputs for the feature extractor network\n",
        "  inputs = Input(inputShape)\n",
        "  # define the first set of CONV => RELU => POOL => DROPOUT layers\n",
        "  x = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  # second set of CONV => RELU => POOL => DROPOUT layers\n",
        "  x = Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Conv2D(256, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        " \n",
        " \t# prepare the final outputs\n",
        "  pooledOutput = GlobalAveragePooling2D()(x)\n",
        "  outputs = Dense(embeddingDim)(pooledOutput)\n",
        "  # build the model\n",
        "  model = Model(inputs, outputs)\n",
        "\t# return the model to the calling function\n",
        "  return model\n",
        "\n",
        "\n",
        "def euclidean_distance(vectors):\n",
        "  (featsA, featsB) = vectors\n",
        "\n",
        "  sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
        "  return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "def plot_training(H):\n",
        "\t# construct a plot that plots and saves the training history\n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "  plt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "  plt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "  plt.title(\"Training Loss and Accuracy\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Loss/Accuracy\")\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPZ1GNZJw2e"
      },
      "source": [
        "#model3 = keras.models.load_model('/content/drive/MyDrive/model2.h5')\n",
        "imgA = Input(inp_shape)\n",
        "imgB = Input(inp_shape)\n",
        "feature = build_siamese_model2()\n",
        "\n",
        "featsA = feature(imgA)\n",
        "featsB = feature(imgB)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYX-5hiqKsbD"
      },
      "source": [
        "from keras.layers import Lambda\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "model3 = Model(inputs=[imgA, imgB], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNi09w4HQXJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491a6c10-4239-4f38-a4ae-004690645dad"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100, 100, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              (None, 128)          197952      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1)            0           model[0][0]                      \n",
            "                                                                 model[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            2           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 197,954\n",
            "Trainable params: 197,954\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8zlZAHLVKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdf1e05-6bb3-4acc-eccc-ed91ef695ec5"
      },
      "source": [
        "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model3.fit([pairImages_np[:,0], pairImages_np[:,1]], pairLabels[:], validation_split=0.2, epochs=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "250/250 [==============================] - 48s 59ms/step - loss: 0.0815 - accuracy: 0.9879 - val_loss: 0.0939 - val_accuracy: 0.9915\n",
            "Epoch 2/4\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0353 - accuracy: 0.9894 - val_loss: 0.0330 - val_accuracy: 0.9915\n",
            "Epoch 3/4\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0228 - accuracy: 0.9899 - val_loss: 0.0212 - val_accuracy: 0.9915\n",
            "Epoch 4/4\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0184 - accuracy: 0.9906 - val_loss: 0.0136 - val_accuracy: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBOL_FZ1JcB"
      },
      "source": [
        "model3.save(\"/content/drive/MyDrive/model4.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdZViekq6cKf"
      },
      "source": [
        "import glob\n",
        "\n",
        "\n",
        "test_dataset2 = []\n",
        "filenames = [img for img in glob.glob('/content/drive/MyDrive/test/test/*.png')]\n",
        "\n",
        "filenames.sort()\n",
        "\n",
        "for img in filenames:\n",
        "  n=cv2.imread(img)\n",
        "  test_dataset2.append(n)\n",
        "\n",
        "for i in range(len(test_dataset2)):\n",
        "  test_dataset2[i] = cv2.resize(test_dataset2[i], dsize=(100, 100), interpolation=cv2.INTER_CUBIC)  \n",
        "  test_dataset2[i] = keras.applications.xception.preprocess_input(\n",
        "    test_dataset2[i], data_format=None\n",
        ")\n",
        "  #test_dataset2[i] =  test_dataset2[i]/255 \n",
        "\n",
        "test_dataset_np = np.asarray(test_dataset2).astype(np.float32)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P48WAgzMQkN4"
      },
      "source": [
        "test_dataset_np2 = []\n",
        "for i in range(len(test_dataset_np)):\n",
        "  test_dataset_np2.append(np.expand_dims(test_dataset_np[i], axis=0))\n",
        "\n",
        "test_dataset_np2 = np.array(test_dataset_np2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ-EHKFMi8eS",
        "outputId": "bf27b30b-1e17-4879-cd2b-529183802898"
      },
      "source": [
        "test_dataset_np2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(204, 1, 100, 100, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yolnm0iiblFi"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxRQs3-l-G8A"
      },
      "source": [
        "predictions = []\n",
        "for i in range(1):\n",
        "  print(i)\n",
        "  for j in range(len(test_dataset_np)):    \n",
        "\n",
        "    preds = model3.predict([test_dataset_np2[i][0], test_dataset_np2[j]])\n",
        "    predictions.append(preds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pShau_kzbZLb"
      },
      "source": [
        "df = pd.DataFrame(columns=['score'])\n",
        "df['score'] = predictions\n",
        "df.to_csv('/content/drive/MyDrive/test_np.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}